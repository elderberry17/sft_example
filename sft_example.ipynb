{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJIqlyI15kj8"
   },
   "source": [
    "## –î–∞—Ç–∞—Å–µ—Ç\n",
    "\n",
    "–ì—Ä—É–∑–∏–º –º–∞–ª—ã—à–∫—É-–¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Å—Ç–∞—Ç—å–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YhYvRDF25j59"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# –≥—Ä—É–∑–∏–º –¥–∞—Ç–∞—Å–µ—Ç –≤–∏–¥–∞ (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è - –æ—Ç–≤–µ—Ç)\n",
    "raw_dataset = load_dataset(\"csujeong/FinancialStockTerms_Eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['QA_text'],\n",
       "        num_rows: 135\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##Question: What is the futures?##Answer: A contract to buy or sell stocks at a certain price at a certain date in the future.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train']['QA_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFYvJUfODabt"
   },
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º —á–∞—Å—Ç—å –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ 80/20 (—Ç—É—Ç –Ω–µ –æ—á–µ–Ω—å —É–º–Ω–æ, –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jX1sIK6X6Opi",
    "outputId": "f0ab2b40-6789-44c4-ece2-bd84e0d5fa65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['QA_text'],\n",
       "        num_rows: 108\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['QA_text'],\n",
       "        num_rows: 27\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# —Å–¥–µ–ª–∞–µ–º —Å–ø–ª–∏—Ç\n",
    "train_indices = range(int(0.8 * len(raw_dataset['train'])))\n",
    "val_indices = range(int(0.8 * len(raw_dataset['train'])), len(raw_dataset['train']))\n",
    "\n",
    "dataset_dict = {\"train\": raw_dataset[\"train\"].select(train_indices),\n",
    "                \"test\": raw_dataset[\"train\"].select(val_indices)}\n",
    "\n",
    "raw_datasets = DatasetDict(dataset_dict)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVRxCJQ76spF"
   },
   "source": [
    "–ì—Ä—É–∑–∏–º PLM, –∫–æ—Ç–æ—Ä—É—é –º—ã —Ö–æ—Ç–∏–º –¥–æ–æ–±—É—á–∏—Ç—å –∏ –µ–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (—Å–≤–æ–π –æ–Ω–∏ –Ω–µ –¥–µ–ª–∞–ª–∏)\n",
    "\n",
    "–ò PLM –∞–≤—Ç–æ—Ä–æ–≤ (–æ–Ω–∏ –≤—Ä–∞–ª–∏!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VvIfUqjK6ntu"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72846f54ef7e4094b1995522dca3c42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0eb1a9c1934dd5982e503edb4e70be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id_falcon = 'tiiuae/falcon-7b'\n",
    "model_id = 'Qwen/Qwen2.5-7B'\n",
    "\n",
    "tokenizer_falcon = AutoTokenizer.from_pretrained(model_id_falcon)\n",
    "model_falcon = AutoModelForCausalLM.from_pretrained(model_id_falcon)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# –∫–æ–µ-—á—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "    tokenizer.model_max_length = 2048\n",
    "\n",
    "if tokenizer_falcon.pad_token_id is None:\n",
    "    tokenizer_falcon.pad_token_id = tokenizer_falcon.eos_token_id\n",
    "\n",
    "if tokenizer_falcon.model_max_length > 100_000:\n",
    "    tokenizer_falcon.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ç–≤–µ—Ç falcon-7b, –∫–æ—Ç–æ—Ä—É—é –¥–æ–æ–±—É—á–∞–ª–∏ –∞–≤—Ç–æ—Ä—ã. –û–Ω–∞ —É–∂–µ —É–º–µ–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/bezgin.aleksey3/llm/llm_venv/lib/python3.11/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is Index?\n",
      "Index is a measure of the performance of a stock market or a stock index\n"
     ]
    }
   ],
   "source": [
    "question = 'What is Index?'\n",
    "\n",
    "question_inputs = tokenizer_falcon(question, return_tensors='pt')\n",
    "outputs = model_falcon.generate(**question_inputs)\n",
    "res = tokenizer_falcon.decode(outputs[0], skip_special_tokens=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ç–≤–µ—Ç QWEN2.5-7B  - –æ–Ω–∞ –ø–æ–∫–∞ –Ω–µ —É–º–µ–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is Index? Index is a list of keywords or topics that are used to locate information in a document, database, or other collection of data. It is a tool that helps users quickly find the information they are looking for by providing a quick reference to the relevant sections or pages. Indexes are commonly found in books, journals, and websites, and they can be created manually or automatically using software.\n"
     ]
    }
   ],
   "source": [
    "question_inputs = tokenizer(question, return_tensors='pt')\n",
    "outputs = model.generate(**question_inputs)\n",
    "res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UNHQsTJ7O6I"
   },
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "–ó–¥–µ—Å—å –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —à–∞–±–ª–æ–Ω—ã –ø—Ä–æ–º–ø—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QA_text']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44kIpOXa7Ep4",
    "outputId": "2895c3ad-d3c6-481f-8f96-64481dfbb977"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5191aab11b54bcd9455fdbec8e097b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=96):   0%|          | 0/108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 27. Reducing num_proc to 27 for dataset of size 27.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a897aef1ec148c2aa0e5d589d81889f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=27):   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 36 of the processed training set:\n",
      "\n",
      "<|im_start|>system\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What is a Stockbroker?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A Stockbroker is a licensed professional who facilitates the buying and selling of stocks on behalf of clients.<|im_end|>\n",
      "\n",
      "Sample 28 of the processed training set:\n",
      "\n",
      "<|im_start|>system\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What is a Reverse Stock Split?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A Reverse Stock Split is the opposite of a regular stock split reducing the number of shares outstanding and increasing the stock price.<|im_end|>\n",
      "\n",
      "Sample 90 of the processed training set:\n",
      "\n",
      "<|im_start|>system\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What is Earnings Per Share (EPS)?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Earnings Per Share (EPS) is a measure of a company's profitability, calculated by dividing net income by the number of outstanding shares. It represents earnings on a per-share basis.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    question, answer = example['QA_text'].split('#Answer: ')        \n",
    "    question = question.replace('##Question: ', '')\n",
    "    question = question.replace('#', '')\n",
    "    answer = answer.replace('#', '')\n",
    "\n",
    "    # —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –Ω–µ –±—É–¥–µ–º –∑–∞–¥–∞–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –µ–≥–æ –≤–ª–∏—è–Ω–∏–µ\n",
    "    messages = [{'role': 'system', 'content': ''},\n",
    "                {'role': 'user', 'content': question},\n",
    "                {'role': 'assistant', 'content': answer}]\n",
    "    \n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        \n",
    "    return example\n",
    "\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "raw_datasets = raw_datasets.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\",)\n",
    "\n",
    "# create the splits\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
    "      print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ, –≤—Å—è–∫–∏–π —Ä–∞–∑, –∫–æ–≥–¥–∞ –º—ã –¥–∞–µ–º –º–æ–¥–µ–ª–∏ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π system, user, assisstant, –µ–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞–µ—Ç —ç—Ç–æ –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "—Ç—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫<|im_end|>\n",
      "<|im_start|>user\n",
      "–∫–∞–∫ –¥–µ–ª–∞ —É –±–∏—Ä–∂–∏ —Å–ø–±?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "–ø–ª–æ—Ö–æ<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{'role': 'system', 'content': '—Ç—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫'},\n",
    "            {'role': 'user', 'content': '–∫–∞–∫ –¥–µ–ª–∞ —É –±–∏—Ä–∂–∏ —Å–ø–±?'},\n",
    "            {'role': 'assistant', 'content': '–ø–ª–æ—Ö–æ'}]\n",
    "\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ–º–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–∏–∑—É–µ–º –º–æ–¥–µ–ª—å –¥–æ bfloat16 –≤–º–µ—Å—Ç–æ float32\n",
    "\n",
    "–ù–µ —Å—Ç–∞–Ω–µ–º –∑–¥–µ—Å—å –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –Ω–∞–¥–æ–ª–≥–æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XrSQuIyu8Rt1"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "model_kwargs = dict(\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë—É–¥–µ–º —É—á–∏—Ç—å —Å –ø–æ–º–æ—â—å—é trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158,
     "referenced_widgets": [
      "ce1f77a753394dc5a25e5470fac18560",
      "7bb6256651a142eabc61984dfe5d379f",
      "12154fd312434260b7f6779a857e1a82",
      "2e44353a59c4480a8e877d842ad16061",
      "abdc9ab22ec049938855373effaf1504",
      "090b3eaf7d2548ee867fc7d9ddf67523",
      "2f2dd26e18ca47dfae4ff33dbb869c0f",
      "e5f6717710074184b78c30f4668be2b5",
      "222a8b16e19140269c44afffbca96865",
      "c3fd940f4dd34ce5b7a462e2bf6f1f71",
      "6264e61a163b4a5dbdb854f7e2ff3056"
     ]
    },
    "id": "W80YklLm_xAY",
    "outputId": "ced661c2-d638-4b4e-bc62-48ca240e2943"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bezgin.aleksey3/llm/llm_venv/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/bezgin.aleksey3/llm/llm_venv/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/bezgin.aleksey3/llm/llm_venv/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "/home/bezgin.aleksey3/llm/llm_venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:175: UserWarning: You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/bezgin.aleksey3/llm/llm_venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:202: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/bezgin.aleksey3/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2.5-7B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/bezgin.aleksey3/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796/model.safetensors.index.json\n",
      "Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e011970b674aa6a8767ea087714a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-7B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/bezgin.aleksey3/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "/home/bezgin.aleksey3/llm/llm_venv/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "/home/bezgin.aleksey3/llm/llm_venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/bezgin.aleksey3/llm/llm_venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a8246a26cb4bf98725ba2c3c73aac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87cfd3e48574151b424ef5fd2f000dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "output_dir = 'data/qwen-7b-finance'\n",
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True, # —É–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Ç–æ–º—É —á—Ç–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–ª–∏\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",      \n",
    "    eval_steps=10,\n",
    "    gradient_accumulation_steps=10,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=5,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=4,\n",
    "    load_best_model_at_end=True,\n",
    "    # save_strategy = \"no\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# –ø—Ä–∏–º–µ–Ω—è–µ–º peft –¥–ª—è CLM\n",
    "# —É–∫–∞–∑—ã–≤–∞–µ–º, –∫ –∫–∞–∫–∏–º —Å–ª–æ—è–º —Ö–æ—Ç–∏–º –ø—Ä–∏–º–µ–Ω–∏—Ç—å\n",
    "peft_config = LoraConfig(\n",
    "        r=256,\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model_id,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        # packing=True,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=tokenizer.model_max_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGldALxQIwYu"
   },
   "source": [
    "## –ó–∞–ø—É—Å–∫–∞–µ–º SFT\n",
    "\n",
    "\n",
    "–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å - —Å—É–º–º–∞—Ä–Ω–∞—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è –ø–æ –≤—ã—Ö–æ–¥–∞–º —Ç–æ–∫–µ–Ω–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "HgEnI5KMIwyt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 108\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 40\n",
      "  Gradient Accumulation steps = 10\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 484,442,112\n",
      "/home/bezgin.aleksey3/llm/llm_venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 05:08, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.843100</td>\n",
       "      <td>4.660173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 27\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to data/qwen-7b-finance/checkpoint-10\n",
      "loading configuration file config.json from cache at /home/bezgin.aleksey3/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in data/qwen-7b-finance/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in data/qwen-7b-finance/checkpoint-10/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from data/qwen-7b-finance/checkpoint-10 (score: 4.660173416137695).\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xxjryHNBKD6"
   },
   "source": [
    "–°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to data/qwen-7b-finance\n",
      "loading configuration file config.json from cache at /home/bezgin.aleksey3/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in data/qwen-7b-finance/tokenizer_config.json\n",
      "Special tokens file saved in data/qwen-7b-finance/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir)  # —É–∫–∞–∑—ã–≤–∞–µ–º –ø–∞–ø–∫—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tCZxj1tBNAc"
   },
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
    "\n",
    "–ó–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ peft —Ç–æ–∂–µ –Ω—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/bezgin.aleksey3/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2.5-7B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/bezgin.aleksey3/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796/model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef0dcbd039b4b18aadcd91a10fe5cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-7B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/bezgin.aleksey3/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "adapter_model_name = 'data/qwen-7b-finance'\n",
    "model_id = 'Qwen/Qwen2.5-7B'\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "sft_model = PeftModel.from_pretrained(base_model, adapter_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = sft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Hkacv5PvBOvE"
   },
   "outputs": [],
   "source": [
    "def get_answer(inputs, tokenizer, model, gen_params={'max_new_tokens': 256}):\n",
    "    inputs_tokenized = tokenizer(inputs, return_tensors='pt')\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **inputs_tokenized.to(model.device),\n",
    "        **gen_params\n",
    "    )\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs_tokenized.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Index is a list of all the words in a document or a collection of documents. It is used to quickly locate the information that is required. Indexing is the process of creating an index. Indexing is done by a computer program called an indexer. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on a collection of documents. Indexing is done on\n"
     ]
    }
   ],
   "source": [
    "print(get_answer('What is Index?', tokenizer, base_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Index is a statistical measure of the changes in the prices of a selected group of goods or services over time, which is used to track the overall level of prices in an economy. It is typically expressed as a percentage change from a base period, and is used to measure inflation, deflation, and economic growth. Indexes can be constructed for a variety of purposes, including tracking the performance of individual sectors or industries, measuring the impact of changes in government policy, and comparing the economic performance of different countries or regions.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer('What is Index?', tokenizer, sft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—â–µ –ø–∞—Ä–∞ —Ç–µ—Å—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A portfolio is a collection of financial assets such as stocks, bonds, mutual funds, and other securities. It is a way for investors to diversify their investments and manage risk by spreading their money across different types of assets. The goal of a portfolio is to achieve a balance between risk and return, and to maximize long\n"
     ]
    }
   ],
   "source": [
    "print(get_answer('What is a Portfolio?', tokenizer, base_model, gen_params={'max_new_tokens': 64}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A portfolio is a collection of financial assets such as stocks, bonds, mutual funds, and cash equivalents. It is a way for investors to diversify their investments and manage risk. The goal of a portfolio is to achieve a balance between risk and return, and to meet the investor's financial goals.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer('What is a Portfolio?', tokenizer, sft_model, gen_params={'max_new_tokens': 64}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Definition & Formula\n",
      "\n",
      "An error occurred trying to load this video.\n",
      "\n",
      "Try refreshing the page, or contact customer support.\n",
      "\n",
      "Coming up next: What is a Circle? - Definition, Area & Properties\n",
      "\n",
      "You're on a roll. Keep up the good work!\n",
      "\n",
      "Take Quiz Watch¬†Next Lesson\n",
      "Your next lesson will play\n"
     ]
    }
   ],
   "source": [
    "print(get_answer('What is a Sector?', tokenizer, base_model, gen_params={'max_new_tokens': 64}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A sector is a portion of a circle. It is bounded by two radii and an arc. The area of a sector is proportional to the area of the circle. The area of a sector is given by the formula: A = (Œ∏/360)œÄr^2, where Œ∏ is the\n"
     ]
    }
   ],
   "source": [
    "print(get_answer('What is a Sector?', tokenizer, sft_model, gen_params={'max_new_tokens': 64}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Options are contracts that give the holder the right, but not the obligation, to buy or sell an underlying asset at a specified price (the strike price) on or before a certain date (the expiration date). There are two main types of options: calls and puts.\n",
      "\n",
      "- A call option gives the holder the right to buy the underlying asset at the strike price.\n",
      "- A put option gives the holder the right to sell the underlying asset at the strike price.\n",
      "\n",
      "Options can be used for various purposes, such as hedging against potential losses, speculating on price movements, or generating income through option writing. They are traded on exchanges and have\n"
     ]
    }
   ],
   "source": [
    "print(get_answer('What are Options?', tokenizer, base_model, gen_params={'max_new_tokens': 128}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Options are contracts that give the holder the right, but not the obligation, to buy or sell an underlying asset at a specified price (the strike price) on or before a certain date (the expiration date). There are two main types of options: calls and puts.\n",
      "\n",
      "- A call option gives the holder the right to buy the underlying asset at the strike price.\n",
      "- A put option gives the holder the right to sell the underlying asset at the strike price.\n",
      "\n",
      "Options can be used for various purposes, such as hedging against potential losses, speculating on price movements, or generating income through option writing. They are traded on exchanges and have\n"
     ]
    }
   ],
   "source": [
    "print(get_answer('What are Options?', tokenizer, sft_model, gen_params={'max_new_tokens': 128}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê –µ—Å–ª–∏ –±—ã –ø—Ä–æ—Å—Ç–æ —É–ª—É—á—à–∏–ª–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏ –ø–æ–ø—Ä–æ—Å–∏–ª–∏ –±—ã –º–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞—Ç—å –≤ –¥–æ–º–µ–Ω–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤?\n",
    "\n",
    "–í–æ–∑–º–æ–∂–Ω–æ, –≤—Å–µ –±—ã–ª–æ –±—ã –¥–∞–∂–µ –ª—É—á—à–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sft_venv",
   "language": "python",
   "name": "sft_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "090b3eaf7d2548ee867fc7d9ddf67523": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12154fd312434260b7f6779a857e1a82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5f6717710074184b78c30f4668be2b5",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_222a8b16e19140269c44afffbca96865",
      "value": 2
     }
    },
    "222a8b16e19140269c44afffbca96865": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2e44353a59c4480a8e877d842ad16061": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3fd940f4dd34ce5b7a462e2bf6f1f71",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6264e61a163b4a5dbdb854f7e2ff3056",
      "value": " 2/2 [00:09&lt;00:00,  4.59s/it]"
     }
    },
    "2f2dd26e18ca47dfae4ff33dbb869c0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6264e61a163b4a5dbdb854f7e2ff3056": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7bb6256651a142eabc61984dfe5d379f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_090b3eaf7d2548ee867fc7d9ddf67523",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2f2dd26e18ca47dfae4ff33dbb869c0f",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "abdc9ab22ec049938855373effaf1504": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3fd940f4dd34ce5b7a462e2bf6f1f71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce1f77a753394dc5a25e5470fac18560": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7bb6256651a142eabc61984dfe5d379f",
       "IPY_MODEL_12154fd312434260b7f6779a857e1a82",
       "IPY_MODEL_2e44353a59c4480a8e877d842ad16061"
      ],
      "layout": "IPY_MODEL_abdc9ab22ec049938855373effaf1504"
     }
    },
    "e5f6717710074184b78c30f4668be2b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
